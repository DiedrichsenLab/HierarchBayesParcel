{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Parcellation Example\n",
    "This jupyter notebook is to demonstrate a minimal example for generating individual cerebellar parcellations using a new individual localization dataset. Usually, the individual data are collected within a relatively short period (e.g. 10 mins). If we generate individual parcellations based on those data directly using some traditional methods, the results are poor and very noisy. However, in the ``HierarchBayesParcel`` framework, the individual parcellations are generated using an optimal integration of a common group prior and the individual localizing data. The main idea of this settings is to \"fill-in\" the knowledge to those uncertain areas with the group prior. \n",
    "\n",
    "The pipeline has two steps: \n",
    "* Train a new emission model for the particular individual localization data. This step can be skipped if you already have a pretrained model for your specific task or resting-state dataset and atlas. \n",
    "* Derive the individual parcellations based on the trained emission model and the group prior.\n",
    "\n",
    "For data import and export we are using the `Functional_Fusion <https://github.com/DiedrichsenLab/Functional_Fusion>`_ Framework, which needs to be installed in addition to the `HierarchBayesParcel` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as pt\n",
    "import nibabel as nb\n",
    "import nitools as nt\n",
    "import matplotlib.pyplot as plt\n",
    "import Functional_Fusion.atlas_map as am\n",
    "import Functional_Fusion.dataset as ds\n",
    "import HierarchBayesParcel.arrangements as ar\n",
    "import HierarchBayesParcel.full_model as fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the the space in which to generate the individual parcellations\n",
    "This step defines the atlas space (e.g. fs32k, SUIT, MNISymC3, etc) - an atlas in Functional_Fusion defines a specific set of brainlocations (grayordinates) that are being sampled. Both the probabilistic atlas and the data need to be read into this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas, _ = am.get_atlas('MNISymC3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the probabilistic group atlas\n",
    "First, we sample the probabilistic group atlas U from a _probseg.nii file at the required brain location. The resultant matrix U has a shape (K by P), where K is the number of parcel and P is the number of brain locations (voxels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the probabilistic atlas at the specific atlas grayordinates\n",
    "atlas_fname = 'atl-NettekovenAsym32_space-MNI152NLin2009cSymC_probseg.nii.gz'\n",
    "U = atlas.read_data(atlas_fname)\n",
    "U = U.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build an arrangement modelÔÉÅ\n",
    "In the `HierarchBayesParcel` the probabilistic atlas is encoded in the `arrangement model`. Depending on whether you want an symmetric or asymmetric individual parcellations, you can choose a `ArrangeIndependent` or `ArrangeIndependentSymmetric` model. The utility function `build_arrangement_model` simply initializes the arrangement model, making sure that NaN and zero values in the `probseg.nii` files are handled correctly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the arrangement model - the parameters are the log-probabilities of the atlas \n",
    "ar_model = ar.build_arrangement_model(U, prior_type='prob', atlas=atlas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load individual localizing data\n",
    "For model training, the data of all subjects needs to be arranged into a num_subj x N x P tensor, where N is the number of observations, and P is the number of brain locations (voxels). To estimate the concentration parameter efficiently, it is useful to have multiple measures of the same conditions. The vector `cond_v` indicates the number of the condition, the vector `part_v` indicates the number of independent data partition (e.g. runs). In this example, we have only two repetitions per condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdtb_dataset = ds.get_dataset_class(':/data/FunctionalFusion','MDTB')\n",
    "subj = mdtb_dataset.get_participants().participant_id\n",
    "data, info = [], []\n",
    "for ses_id in mdtb_dataset.sessions:\n",
    "        this_data = []\n",
    "        this_info = []\n",
    "        info.append(mdtb_dataset.get_info(ses_id=ses_id, type='CondHalf'))\n",
    "        for i, s in enumerate(subj):\n",
    "                file_name = f'/{s}_space-{atlas.name}_{ses_id}_CondHalf.dscalar.nii'\n",
    "                this_data.append(atlas.read_data(data_dir.format(s) + file_name).T)\n",
    "        data.append(np.stack(this_data))\n",
    "Now, we assemble condition and partition vectors. cond_v is a list of 1d array to indicate the condition numbers for dimension N, and part_v is a list of 1d array to specify the partitioning (runs, or repeated measurement for example) of a data tensor. sub_ind is to indicate the unique subjects index for each data tensor, repeated subjects across data tensors are theoretically allowed.\n",
    "\n",
    "cond_v, part_v, sub_ind = [], [], []\n",
    "for j, inf in enumerate(info):\n",
    "        cond_v.append(inf['cond_num_uni'].values.reshape(-1,))\n",
    "        part_v.append(inf['half'].values.reshape(-1,))\n",
    "        sub_ind.append(np.arange(0, len(subj)))\n",
    "Here, the length of the four outputs should have same length. This length is the number of emission models in your training model.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
