{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Parcellation Example\n",
    "This jupyter notebook is to demonstrate a minimal example for generating individual cerebellar parcellations using a new individual localization dataset. Usually, the individual data are collected within a relatively short period (e.g. 10 mins). If we generate individual parcellations based on those data directly using some traditional methods, the results are poor and very noisy. However, in the ``HierarchBayesParcel`` framework, the individual parcellations are generated using an optimal integration of a common group prior and the individual localizing data. The main idea of this settings is to \"fill-in\" the knowledge to those uncertain areas with the group prior. \n",
    "\n",
    "The pipeline has two steps: \n",
    "* Train a new emission model for the particular individual localization data. This step can be skipped if you already have a pretrained model for your specific task or resting-state dataset and atlas. \n",
    "* Derive the individual parcellations based on the trained emission model and the group prior.\n",
    "\n",
    "For data import and export we are using the `Functional_Fusion <https://github.com/DiedrichsenLab/Functional_Fusion>`_ Framework, which needs to be installed in addition to the `HierarchBayesParcel` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as pt\n",
    "import nibabel as nb\n",
    "import nitools as nt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import Functional_Fusion.atlas_map as am\n",
    "import Functional_Fusion.dataset as ds\n",
    "import HierarchBayesParcel.arrangements as ar\n",
    "import HierarchBayesParcel.full_model as fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the the space in which to generate the individual parcellations\n",
    "This step defines the atlas space (e.g. fs32k, SUIT, MNISymC3, etc) - an atlas in Functional_Fusion defines a specific set of brainlocations (grayordinates) that are being sampled. Both the probabilistic atlas and the data need to be read into this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas, _ = am.get_atlas('MNISymC3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the probabilistic group atlas\n",
    "First, we sample the probabilistic group atlas U from a _probseg.nii file at the required brain location. The resultant matrix U has a shape (K by P), where K is the number of parcel and P is the number of brain locations (voxels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the probabilistic atlas at the specific atlas grayordinates\n",
    "atlas_fname = 'atl-NettekovenAsym32_space-MNI152NLin2009cSymC_probseg.nii.gz'\n",
    "U = atlas.read_data(atlas_fname)\n",
    "U = U.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build an arrangement modelÔÉÅ\n",
    "In the `HierarchBayesParcel` the probabilistic atlas is encoded in the `arrangement model`. Depending on whether you want an symmetric or asymmetric individual parcellations, you can choose a `ArrangeIndependent` or `ArrangeIndependentSymmetric` model. The utility function `build_arrangement_model` simply initializes the arrangement model, making sure that NaN and zero values in the `probseg.nii` files are handled correctly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jdiedrichsen/Python/HierarchBayesParcel/HierarchBayesParcel/arrangements.py:1931: UserWarning: The marginal probability has 4 voxels NaN value - replacing with flat distribution\n",
      "/Users/jdiedrichsen/Python/HierarchBayesParcel/HierarchBayesParcel/arrangements.py:1938: UserWarning: The marginal probability has 5398 voxels zero values - adding small value to avoid -inf\n"
     ]
    }
   ],
   "source": [
    "# Build the arrangement model - the parameters are the log-probabilities of the atlas \n",
    "ar_model = ar.build_arrangement_model(U, prior_type='prob', atlas=atlas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load individual localizing data\n",
    "For model training, the data of all subjects needs to be arranged into a num_subj x N x P tensor, where N is the number of observations, and P is the number of brain locations (voxels). To estimate the concentration parameter efficiently, it is useful to have multiple measures of the same conditions. In this example, we have only two repetitions per condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data_mdtb'\n",
    "# Read the participant tsv file to get the name of the available subjects\n",
    "subj_info = pd.read_csv(f'{data_dir}/participants.tsv',sep='\\t')\n",
    "data = []\n",
    "# Read the data for each subject\n",
    "for i, s in enumerate(subj_info['participant_id']):\n",
    "        file_name = f'{data_dir}/{s}_space-{atlas.name}_ses-s1_CondHalf.dscalar.nii'\n",
    "        datafile = nb.load(file_name)\n",
    "        data.append(datafile.get_fdata())\n",
    "# make the numsubj x numcond x numvoxel tensor\n",
    "data = np.stack(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector `cond_v` indicates the number of the condition, the vector `part_v` indicates the number of independent data partition (e.g. runs).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv(f'{data_dir}/info-CondHalf.tsv',sep='\\t')\n",
    "cond_v = info['cond_num_uni'].values\n",
    "part_v = info['half'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,  1,  2,  3,  4,  5,\n",
       "        6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22,\n",
       "       23, 24, 25, 26, 27, 28, 29])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
