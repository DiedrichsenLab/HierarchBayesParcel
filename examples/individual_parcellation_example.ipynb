{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Parcellation Example\n",
    "This jupyter notebook is to demonstrate a minimal example for generating individual cerebellar parcellations using a new individual localization dataset. Usually, the individual data are collected within a relatively short period (e.g. 10 mins). If we generate individual parcellations based on those data directly using some traditional methods, the results are poor and very noisy. In the ``HierarchBayesParcel`` framework, the individual parcellations are generated using an optimal integration of a common group prior and the individual localizing data. The main idea is to regularize the noisy individual parcellation estimate towards the group map (the prior) just by the right amount.  \n",
    "\n",
    "The pipeline has two main steps: \n",
    "* Train a new emission model for the particular individual localization data. This step can be skipped if you already have a pretrained model for your specific task or resting-state dataset and atlas. \n",
    "* Derive the individual parcellations based on the trained emission model and the group prior.\n",
    "\n",
    "For data import and export we are using the `Functional_Fusion <https://github.com/DiedrichsenLab/Functional_Fusion>`_ Framework, which needs to be installed in addition to the `HierarchBayesParcel` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch as pt\n",
    "import nibabel as nb\n",
    "import nitools as nt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import Functional_Fusion.atlas_map as am\n",
    "import Functional_Fusion.dataset as ds\n",
    "import HierarchBayesParcel.arrangements as ar\n",
    "import HierarchBayesParcel.emissions as em\n",
    "import HierarchBayesParcel.full_model as fm\n",
    "import HierarchBayesParcel.util as ut\n",
    "import SUITPy as suit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Define the the space in which to generate the individual parcellations\n",
    "This step defines the atlas space (e.g. fs32k, SUIT, MNISymC3, etc) - an atlas in Functional_Fusion defines a specific set of brainlocations (grayordinates) that are being sampled. Both the probabilistic atlas and the data need to be read into this space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas, _ = am.get_atlas('MNISymC3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load the probabilistic group atlas\n",
    "First, we sample the probabilistic group atlas U from a _probseg.nii file at the required brain location. The resultant matrix U has a shape (K by P), where K is the number of parcel and P is the number of brain locations (voxels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the probabilistic atlas at the specific atlas grayordinates\n",
    "atlas_fname = 'atl-NettekovenAsym32_space-MNI152NLin2009cSymC_probseg.nii.gz'\n",
    "U = atlas.read_data(atlas_fname)\n",
    "U = U.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Build an arrangement modelÔÉÅ\n",
    "In the `HierarchBayesParcel` the probabilistic atlas is encoded in the `arrangement model`. Depending on whether you want an symmetric or asymmetric individual parcellations, you can choose a `ArrangeIndependent` or `ArrangeIndependentSymmetric` model. The utility function `build_arrangement_model` simply initializes the arrangement model, making sure that NaN and zero values in the `probseg.nii` files are handled correctly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jdiedrichsen/Python/HierarchBayesParcel/HierarchBayesParcel/arrangements.py:1867: UserWarning: The marginal probability has 4 voxels NaN value - replacing with flat distribution\n",
      "  warnings.warn(f'The marginal probability has {nan_voxl} voxels '\n",
      "/Users/jdiedrichsen/Python/HierarchBayesParcel/HierarchBayesParcel/arrangements.py:1874: UserWarning: The marginal probability has 5398 voxels zero values - adding small value to avoid -inf\n",
      "  warnings.warn(f'The marginal probability has {zero_voxl} voxels'\n"
     ]
    }
   ],
   "source": [
    "# Build the arrangement model - the parameters are the log-probabilities of the atlas\n",
    "ar_model = ar.build_arrangement_model(U, prior_type='prob', atlas=atlas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load individual localizing data\n",
    "For model training, the data of all subjects needs to be arranged into a num_subj x N x P tensor, where N is the number of observations, and P is the number of brain locations (voxels). To estimate the concentration parameter efficiently, it is useful to have multiple measures of the same conditions. In this example, we have only two repetitions per condition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data_mdtb'\n",
    "# Read the participant tsv file to get the name of the available subjects\n",
    "subj_info = pd.read_csv(f'{data_dir}/participants.tsv',sep='\\t')\n",
    "data = []\n",
    "# Read the data for each subject\n",
    "for i, s in enumerate(subj_info['participant_id']):\n",
    "        file_name = f'{data_dir}/{s}_space-{atlas.name}_ses-s1_CondHalf.dscalar.nii'\n",
    "        datafile = nb.load(file_name)\n",
    "        data.append(datafile.get_fdata())\n",
    "# make the numsubj x numcond x numvoxel tensor\n",
    "data = np.stack(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vector `cond_v` indicates the number of the condition, the vector `part_v` indicates the number of independent data partition (e.g. runs).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = pd.read_csv(f'{data_dir}/info-CondHalf.tsv',sep='\\t')\n",
    "cond_v = info['cond_num_uni'].values\n",
    "part_v = info['half'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Fit the emission model to the data\n",
    "For a new localization dataset, we need to fit the emission model to the data. In this step, we estimation the average functional profiles for each region ($V$) and the concentration parameter ($\\kappa$) for the von Mises-Fisher distribution.\n",
    "Of course, we want to keep the atlas the same, so the arrangement model will not be fit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K is the number of parcels\n",
    "K = ar_model.K\n",
    "# Make a design matrix\n",
    "X= ut.indicator(cond_v)\n",
    "# Build an emission model\n",
    "em_model = em.MixVMF(K=K,P=atlas.P, X=X,part_vec=part_v)\n",
    "# Build the full model: The emission models are passed as a list, as usually we have multiple data sets\n",
    "M = fm.FullMultiModel(ar_model, [em_model])\n",
    "# Attach the data to the model - this is done for speed\n",
    "# The data is passed as a list with on element per data set\n",
    "M.initialize([data])\n",
    "\n",
    "# Now we can run the EM algorithm\n",
    "M, ll, _, U_indiv = M.fit_em(iter=200, tol=0.01,\n",
    "    fit_arrangement=False,fit_emission=True,first_evidence=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Get the individual parcellation for a new subject\n",
    "The last step already returns the individual parcellation for all the subjects in the localizer dataset. However, if you ever want to use a trained emission model to generate individual parcellations for a new subject, you would simply attach the new data and then run a simple Estep. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "M.initialize([data])\n",
    "U_indiv,_ = M.Estep()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize the individual parcellations\n",
    "In the last step, we project the individual parcellations back into Volume space. \n",
    "We then use the SUITPy toolbox to visualize these on the surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "plot() got an unexpected keyword argument 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sz/7lqgfrln5738xc_hktty2sb00000gp/T/ipykernel_27956/813604170.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_lut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'atl-NettekovenAsym32.lut'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msurf_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m ax = suit.flatmap.plot(label,\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'plotly'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: plot() got an unexpected keyword argument 'label'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Make a nifti image of the first subject\n",
    "nifti = atlas.data_to_nifti(U)\n",
    "# Project the nifti image to the surface over the MNISymC space\n",
    "surf_data = suit.flatmap.vol_to_surf(nifti, stats='nanmean',space='MNISymC')\n",
    "# Load colormap and label names \n",
    "lid,cmap, names = nt.read_lut('atl-NettekovenAsym32.lut')\n",
    "label = np.argmax(surf_data, axis=1)+1\n",
    "ax = suit.flatmap.plot(label,\n",
    "    render='plotly',\n",
    "    cmap=cmap,\n",
    "    cscale=[0,31],\n",
    "    label_names = names,\n",
    "    new_figure=False,\n",
    "    overlay_type='label',\n",
    "    bordersize=4,\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5035, 0.3072, 0.6261],\n",
       "       [0.3201, 0.7377, 0.3657],\n",
       "       [0.5035, 0.3072, 0.6261],\n",
       "       [0.3201, 0.7377, 0.3657]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmap[[1,16,17,32],:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
